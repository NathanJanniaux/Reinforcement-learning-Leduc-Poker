{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    qtable=[]\n",
    " # Agent class definition   state=None\n",
    "    learning_rate=0.1\n",
    "    \n",
    "    state=None #0-5\n",
    "    \n",
    "    actions=[]\n",
    "    state_number=6\n",
    "    \n",
    "    perf=[]\n",
    "    \n",
    "    def __init__(self):\n",
    "        #qtable creation\n",
    "        self.qtable=np.zeros((self.state_number,len(game.actions)))\n",
    "        self.perf=[]\n",
    "        \n",
    "    #allow to print leduc game state\n",
    "    def __str__(self):\n",
    "        return \"State = {} \\nQTable = {} \\nLearning rate = {}\".format(self.state,self.qtable,self.learning_rate)\n",
    "    \n",
    "    def explore_action(self):\n",
    "        action=random.randrange(0,2)\n",
    "        return action\n",
    "    \n",
    "    def exploit_action(self):\n",
    "        action=utils.get_max_list(self.qtable[self.state])\n",
    "        return action\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        self.state=state\n",
    "        \n",
    "    def set_qtable(self,qtable):\n",
    "        self.qtable=qtable\n",
    "        \n",
    "    def set_perf(self,perf):\n",
    "        self.perf.append(perf)\n",
    "\n",
    "    def update(self,action,reward):\n",
    "        if(self.state!=-1):\n",
    "            new_value = (1 - self.learning_rate) * self.qtable[self.state, action] +  self.learning_rate * reward\n",
    "            self.qtable[self.state, action] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandAgent():\n",
    "    \n",
    "    action=None\n",
    "           \n",
    "        \n",
    "    def __init__(self):\n",
    "        #qtable creation\n",
    "        self.set_action(0)\n",
    "        \n",
    "    def set_action(self,hand):\n",
    "        self.action=random.randrange(0,2)\n",
    "        \n",
    "    def get_action(self):\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveAgent():\n",
    "    \n",
    "    action=None\n",
    "        \n",
    "    def set_action(self,hand):\n",
    "        \n",
    "        if(hand==0):\n",
    "            self.action=0\n",
    "        elif(hand==2):\n",
    "            self.action=1\n",
    "        else:\n",
    "            self.action=random.randrange(0,2)\n",
    "        \n",
    "    def get_action(self):\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weird Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeirdAgent():\n",
    "    \n",
    "    action=None\n",
    "        \n",
    "    def set_action(self,hand):\n",
    "        \n",
    "        if(hand==0):\n",
    "            self.action=0\n",
    "        elif(hand==1):\n",
    "            self.action=random.randrange(0,2)\n",
    "        elif(hand==2):\n",
    "            self.action=0\n",
    "        \n",
    "    def get_action(self):\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bluff Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BluffAgent():\n",
    "    \n",
    "    action=None\n",
    "        \n",
    "    def set_action(self,hand):\n",
    "        \n",
    "        if(hand==0):\n",
    "            if(random.randrange(0,101)<=15): #15% of the time, when it has a J, the agent will bluff and push\n",
    "                self.action=1\n",
    "            else:\n",
    "                self.action=0\n",
    "        elif(hand==1):\n",
    "            if(random.randrange(0,101)<=60): #60% of the time, when it has a Q, the agent will fold\n",
    "                self.action=0\n",
    "            else:\n",
    "                self.action=1\n",
    "        elif(hand==2):\n",
    "            self.action=1\n",
    "        \n",
    "    def get_action(self):\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumpAgent():\n",
    "    \n",
    "    action=None\n",
    "        \n",
    "  \n",
    "        \n",
    "    def set_action(self,hand):\n",
    "        \n",
    "        if(hand==0):\n",
    "            self.action=0\n",
    "        elif(hand==2):\n",
    "            self.action=0\n",
    "        else:\n",
    "            self.action=0\n",
    "        \n",
    "    def get_action(self):\n",
    "        return self.action\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeducGame class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeducGame:\n",
    "    deck = []\n",
    "    actions = [0,1] #0 is fold and 1 is push\n",
    "    firstplayer=None; #0 if player1 and 1 if player2\n",
    "    hand_player1=0;\n",
    "    hand_player2=0;\n",
    "    boardcard=0;\n",
    "    result=0;\n",
    "    \n",
    "    #inititate a game\n",
    "    def __init__(self):\n",
    "        self.deck = [0,0,1,1,2,2]\n",
    "        \n",
    "        #deal card to game from deck\n",
    "        self.hand_player1=utils.choose_and_remove(self.deck)\n",
    "        self.hand_player2=utils.choose_and_remove(self.deck)\n",
    "        self.boardcard=utils.choose_and_remove(self.deck)\n",
    "        self.result=self.get_result()\n",
    "        self.firstplayer=random.randrange(0,2)\n",
    "        \n",
    "    #allow to print leduc game state\n",
    "    def __str__(self):\n",
    "        return \"Player1 = {} \\nPlayer2 = {} \\nBoard = {} \\nDeck = {}\\nResult = {}\".format(self.hand_player1,self.hand_player2,self.boardcard,self.deck, self.result)\n",
    "     \n",
    "    def get_firstplayer(self):\n",
    "            return self.firstplayer\n",
    "        \n",
    "    def get_hand_player1(self):\n",
    "            return self.hand_player1\n",
    "    \n",
    "    def get_hand_player2(self):\n",
    "            return self.hand_player2\n",
    "    \n",
    "    #result() : \n",
    "    # 0  -> draw\n",
    "    # 1  -> player1 win\n",
    "    #-1  -> player2 win\n",
    "    \n",
    "    def get_result(self):\n",
    "        #Pairs\n",
    "        if (self.hand_player1==self.boardcard):\n",
    "            result=1\n",
    "        elif (self.hand_player2==self.boardcard):\n",
    "            result=-1\n",
    "        #Highest card\n",
    "        elif (self.hand_player1>self.hand_player2):\n",
    "            result=1\n",
    "        elif(self.hand_player1<self.hand_player2):\n",
    "            result=-1\n",
    "        #Draw\n",
    "        else:\n",
    "            result=0\n",
    "        return result        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LeducGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'choose_and_remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-53ed46c7ba3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLeducGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-557874b3effd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#deal card to game from deck\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhand_player1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_and_remove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhand_player2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_and_remove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboardcard\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_and_remove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'choose_and_remove'"
     ]
    }
   ],
   "source": [
    "game=LeducGame()\n",
    "print(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    agent=None\n",
    "    game=None\n",
    "    stack=None\n",
    "    \n",
    "    def __init__(self,agent):\n",
    "        self.game=LeducGame()\n",
    "        self.agent=agent\n",
    "        self.stack=10\n",
    "        self.agent.set_action(self.game.get_hand_player2())\n",
    "        self.opponent_action=self.agent.get_action()\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{} \\nOpponent_Action = {}\\nStack = {}\".format(self.game,self.agent.get_action(),self.stack)\n",
    "    \n",
    "    def get_state(self):\n",
    "        hand1=self.game.get_hand_player1()\n",
    "        first=self.game.get_firstplayer()\n",
    "        \n",
    "        if first is 0:\n",
    "            return hand1\n",
    "        elif(first==1 and self.opponent_action==1):\n",
    "            return hand1+3\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def reward(self,qagent_action,state):\n",
    "        r=0\n",
    "        \n",
    "        if(qagent_action==0):\n",
    "            r=-0.1*self.stack\n",
    "        elif(self.opponent_action==0):\n",
    "            r=0.1*self.stack\n",
    "        elif(qagent_action==1 and self.opponent_action==1):\n",
    "            if(self.game.get_result()==1):\n",
    "                r=self.stack\n",
    "            elif(self.game.get_result()==-1):\n",
    "                r=-self.stack\n",
    "\n",
    "        return r\n",
    "    \n",
    "    def reset(self):\n",
    "        self.game=LeducGame() \n",
    "        self.stack=10\n",
    "        self.agent.set_action(self.game.get_hand_player2())\n",
    "        self.opponent_action=self.agent.get_action()\n",
    "        \n",
    "    def test(self,n):\n",
    "        r=0\n",
    "        for i in range(n):\n",
    "            r=r+self.reward(self,qagent_action,state) \n",
    "            self.reset() \n",
    "        r=r/n\n",
    "        return r\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=1000\n",
    "evaluate_every=30\n",
    "test_number=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAgent VS RandAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randAgent=RandAgent()\n",
    "\n",
    "env=Environment(randAgent)\n",
    "envTest=Environment(randAgent)\n",
    "qagent=QAgent()\n",
    "\n",
    "for i in range(epochs_number): \n",
    "    #test\n",
    "    if(i % evaluate_every == 0):\n",
    "        r=0\n",
    "        for j in range(test_number):\n",
    "            envTest.reset()\n",
    "            qagent.set_state(envTest.get_state())\n",
    "            test_qagent_action=qagent.exploit_action()\n",
    "            r=r+envTest.reward(test_qagent_action,envTest.get_state())\n",
    "        perf=float(r/test_number)\n",
    "        qagent.set_perf(perf)\n",
    "    \n",
    "    env.reset()\n",
    "    qagent.set_state(env.get_state())\n",
    "    qagent_action=qagent.explore_action()\n",
    "    reward=env.reward(qagent_action,env.get_state())\n",
    "    qagent.update(qagent_action,reward)\n",
    "    \n",
    "print(qagent.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qagent.perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAgent VS NaiveAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveAgent=NaiveAgent()\n",
    "\n",
    "env=Environment(naiveAgent)\n",
    "envTest=Environment(naiveAgent)\n",
    "qagent=QAgent()\n",
    "\n",
    "for i in range(epochs_number): \n",
    "    #test\n",
    "    if(i % evaluate_every == 0):\n",
    "        r=0\n",
    "        for j in range(test_number):\n",
    "            envTest.reset()\n",
    "            qagent.set_state(envTest.get_state())\n",
    "            test_qagent_action=qagent.exploit_action()\n",
    "            r=r+envTest.reward(test_qagent_action,envTest.get_state())\n",
    "        perf=float(r/test_number)\n",
    "        qagent.set_perf(perf)\n",
    "    \n",
    "    env.reset()\n",
    "    qagent.set_state(env.get_state())\n",
    "    qagent_action=qagent.explore_action()\n",
    "    reward=env.reward(qagent_action,env.get_state())\n",
    "    qagent.update(qagent_action,reward)\n",
    "    \n",
    "print(qagent.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qagent.perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAgent VS WeirdAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdAgent=WeirdAgent()\n",
    "\n",
    "env=Environment(weirdAgent)\n",
    "envTest=Environment(weirdAgent)\n",
    "qagent=QAgent()\n",
    "\n",
    "for i in range(epochs_number): \n",
    "    #test\n",
    "    if(i % evaluate_every == 0):\n",
    "        r=0\n",
    "        for j in range(test_number):\n",
    "            envTest.reset()\n",
    "            qagent.set_state(envTest.get_state())\n",
    "            test_qagent_action=qagent.exploit_action()\n",
    "            r=r+envTest.reward(test_qagent_action,envTest.get_state())\n",
    "        perf=float(r/test_number)\n",
    "        qagent.set_perf(perf)\n",
    "    \n",
    "    env.reset()\n",
    "    qagent.set_state(env.get_state())\n",
    "    qagent_action=qagent.explore_action()\n",
    "    reward=env.reward(qagent_action,env.get_state())\n",
    "    qagent.update(qagent_action,reward)\n",
    "    \n",
    "print(qagent.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qagent.perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAgent VS BluffAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluffAgent=BluffAgent()\n",
    "\n",
    "env=Environment(bluffAgent)\n",
    "envTest=Environment(bluffAgent)\n",
    "qagent=QAgent()\n",
    "\n",
    "for i in range(epochs_number): \n",
    "    #test\n",
    "    if(i % evaluate_every == 0):\n",
    "        r=0\n",
    "        for j in range(test_number):\n",
    "            envTest.reset()\n",
    "            qagent.set_state(envTest.get_state())\n",
    "            test_qagent_action=qagent.exploit_action()\n",
    "            r=r+envTest.reward(test_qagent_action,envTest.get_state())\n",
    "        perf=float(r/test_number)\n",
    "        qagent.set_perf(perf)\n",
    "    \n",
    "    env.reset()\n",
    "    qagent.set_state(env.get_state())\n",
    "    qagent_action=qagent.explore_action()\n",
    "    reward=env.reward(qagent_action,env.get_state())\n",
    "    qagent.update(qagent_action,reward)\n",
    "    \n",
    "print(qagent.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qagent.perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAgent VS DumpAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpAgent=DumpAgent()\n",
    "\n",
    "env=Environment(dumpAgent)\n",
    "envTest=Environment(dumpAgent)\n",
    "qagent=QAgent()\n",
    "\n",
    "for i in range(epochs_number): \n",
    "    #test\n",
    "    if(i % evaluate_every == 0):\n",
    "        r=0\n",
    "        for j in range(test_number):\n",
    "            envTest.reset()\n",
    "            qagent.set_state(envTest.get_state())\n",
    "            test_qagent_action=qagent.exploit_action()\n",
    "            r=r+envTest.reward(test_qagent_action,envTest.get_state())\n",
    "        perf=float(r/test_number)\n",
    "        qagent.set_perf(perf)\n",
    "    \n",
    "    env.reset()\n",
    "    qagent.set_state(env.get_state())\n",
    "    qagent_action=qagent.explore_action()\n",
    "    reward=env.reward(qagent_action,env.get_state())\n",
    "    qagent.update(qagent_action,reward)\n",
    "    \n",
    "print(qagent.qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(qagent.perf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
